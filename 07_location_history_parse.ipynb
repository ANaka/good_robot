{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp location_history_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# location_history_parse\n",
    "\n",
    "I definitely jacked this from somewhere online but don't remember where now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old functions I jacked from somewhere online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_json_file(filename):\n",
    "    with open(filename) as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_activities(activity_recs):\n",
    "    flat_activities = {}\n",
    "    for activity_rec in activity_recs:\n",
    "        for activity in activity_rec['activity']:\n",
    "            flat_activities[activity['type']] = activity['confidence']\n",
    "    return flat_activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_json_file(filename, filters = {}):\n",
    "    raw_json_data = load_json_file(filename)\n",
    "\n",
    "    parsed_json_data = parse_json_data(raw_json_data)\n",
    "\n",
    "    if filters != {}:\n",
    "        filtered_json_data = filter_json_data(parsed_json_data, filters=filters)\n",
    "    else:\n",
    "        filtered_json_data = parsed_json_data\n",
    "\n",
    "    sorted_json_data = sort_json_data(filtered_json_data, 'timestamp')\n",
    "\n",
    "    return sorted_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_json_file_as_rows(filename, filters = {}):\n",
    "    sorted_json_data = parse_json_file(filename, filters)\n",
    "    rows = rowify_json_data(sorted_json_data)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_json_data(raw_json_data):\n",
    "    parsed_data_points = []\n",
    "    for data_point in raw_json_data[\"locations\"]:\n",
    "        parsed_data_points.append(parse_data_point(data_point))\n",
    "    return {\"locations\": parsed_data_points}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_data_point(data_point):\n",
    "    data_point[\"timestamp\"] = parse_timestamp(data_point.get(\"timestampMs\"))\n",
    "    data_point[\"lat\"] = data_point.get(\"latitudeE7\") / 1e7\n",
    "    data_point[\"lon\"] = data_point.get(\"longitudeE7\") / 1e7\n",
    "    data_point[\"accuracy\"] = data_point.get(\"accuracy\", np.nan)\n",
    "    data_point[\"velocity\"] = data_point.get(\"velocity\", np.nan)\n",
    "    data_point[\"heading\"] = data_point.get(\"heading\", np.nan)\n",
    "    data_point[\"altitude\"] = data_point.get(\"altitude\", np.nan)\n",
    "    data_point[\"vertical_accuracy\"] = data_point.get(\"verticalAccuracy\", np.nan)\n",
    "#     data_point[\"activity\"], data_point[\"activity_confidence\"] = parse_activity(data_point.get(\"activity\", \"\"))\n",
    "    return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_activity(activity):\n",
    "    if activity == \"\":\n",
    "        return [\"\", \"\"]\n",
    "    first_activity = activity[0]\n",
    "    classifications = first_activity[\"activity\"]\n",
    "    best_classification = classifications[0]\n",
    "    return [best_classification[\"type\"], best_classification[\"confidence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def filter_json_data(parsed_json_data, filters={}):\n",
    "    filtered_data_points = []\n",
    "    for data_point in parsed_json_data[\"locations\"]:\n",
    "        if check_against_filters(data_point, filters):\n",
    "            filtered_data_points.append(data_point)\n",
    "    return {\"locations\": filtered_data_points}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sort_json_data(filtered_json_data, sort_key=\"timestamp\"):\n",
    "    filtered_json_data[\"locations\"].sort(key=lambda data_point: data_point[sort_key])\n",
    "    return filtered_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def rowify_json_data(sorted_json_data):\n",
    "    headers = [\n",
    "        \"timestamp\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"accuracy\",\n",
    "        \"velocity\",\n",
    "        \"heading\",\n",
    "        \"altitude\",\n",
    "        \"vertical_accuracy\",\n",
    "        \"activity\",\n",
    "        \"activity_confidence\",\n",
    "    ]\n",
    "    rows = [headers]\n",
    "    for data_point in sorted_json_data[\"locations\"]:\n",
    "        row = [\n",
    "            str(data_point.get(\"timestamp\")),\n",
    "            data_point[\"lat\"],\n",
    "            data_point[\"lon\"],\n",
    "            data_point[\"accuracy\"],\n",
    "            data_point[\"velocity\"],\n",
    "            data_point[\"heading\"],\n",
    "            data_point[\"altitude\"],\n",
    "            data_point[\"vertical_accuracy\"],\n",
    "            data_point[\"activity\"],\n",
    "            data_point[\"activity_confidence\"],\n",
    "        ]\n",
    "        rows.append(row)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_timestamp(t):\n",
    "    return datetime.utcfromtimestamp(int(t) / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def check_against_filters(data_point, filters):\n",
    "    start = filters.get(\"start\", False)\n",
    "    end = filters.get(\"end\", False)\n",
    "    bbox = filters.get(\"bbox\", False)\n",
    "\n",
    "    # Skip data from before the provided start datetime\n",
    "    if start and (data_point[\"timestamp\"] <= start):\n",
    "        return False\n",
    "\n",
    "    # Skip data from after the provided end datetime\n",
    "    if end and (end < data_point[\"timestamp\"]):\n",
    "        return False\n",
    "\n",
    "    # Skip data_points outside of bounding box\n",
    "    if (bbox\n",
    "        and ((data_point[\"lat\"] < bbox[\"min_lat\"])\n",
    "            or (data_point[\"lat\"] > bbox[\"max_lat\"])\n",
    "            or (data_point[\"lon\"] < bbox[\"min_lon\"])\n",
    "            or (data_point[\"lon\"] > bbox[\"max_lon\"]))):\n",
    "        return False\n",
    "\n",
    "    # Skip data that hasn't been assigned an activity category\n",
    "    if len(data_point[\"activity\"]) == 0:\n",
    "        return False\n",
    "\n",
    "    # If data_point passes all filters, return True\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def d(timestamp):\n",
    "    return str(parse_timestamp(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_timestamp(datetime):\n",
    "    return datetime.strftime(\"%s%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def t(datetime):\n",
    "    return create_timestamp(datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new functions for parsing to pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_google_location_history_json(filepath):\n",
    "    raw = load_json_file(filepath)\n",
    "    raw = raw['locations']\n",
    "    parsed_data_points = []\n",
    "    for data_point in raw:\n",
    "        parsed_data_point = parse_data_point(data_point)\n",
    "        if 'activity' in data_point.keys():\n",
    "            parsed_activities = parse_activities(data_point['activity'])\n",
    "            parsed_data_point.update(parsed_activities)\n",
    "            parsed_data_point.pop('activity')\n",
    "        parsed_data_points.append(parsed_data_point)\n",
    "    return parsed_data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parsed_loc_hist_to_df(parsed_data_points):\n",
    "    df = pd.DataFrame().from_records(parsed_data_points)\n",
    "    \n",
    "    #calculate odds of being moving\n",
    "    df['MOVING'] = 100 - df.loc[:,['STILL','UNKNOWN']].sum(axis=1)\n",
    "    df.loc[df['STILL'].isna() & df['UNKNOWN'].isna(), 'MOVING'] = np.nan\n",
    "    \n",
    "    #add delta position\n",
    "    df.loc[1:,'dlat'] = df['lat'].diff()\n",
    "    df.loc[1:,'dlon'] = df['lon'].diff()\n",
    "    df['dpos'] = (df['dlat']**2 + df['dlon']**2) ** 0.5\n",
    "    \n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    df = df.rename(columns={'timestamp':'t'})\n",
    "    \n",
    "    front_cols = [\n",
    "        't',\n",
    "        'lat',\n",
    "        'lon',\n",
    "        'dpos',\n",
    "        'velocity',\n",
    "        'heading',\n",
    "        'moving',\n",
    "        'still',\n",
    "        'unknown',\n",
    "        'in_vehicle',\n",
    "        'on_bicycle',\n",
    "        'on_foot',\n",
    "        'walking',\n",
    "        'running',\n",
    "        'dlat',\n",
    "        'dlon',\n",
    "    ]\n",
    "    cols = df.columns.to_list()\n",
    "    for col in reversed(front_cols):\n",
    "        cols.remove(col)\n",
    "        cols.insert(0,col)\n",
    "    df = df.loc[:, cols]\n",
    "    \n",
    "    df = df.set_index('t')\n",
    "#     df.index = df.index.tz_localize('UTC', ambiguous='infer').tz_convert('US/Pacific')\n",
    "    df.index = df.index - pd.Timedelta('8H')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def google_location_history_to_df(filepath):\n",
    "    return parsed_loc_hist_to_df(parse_google_location_history_json(filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for adding new info to parsed df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def haversine(coord1, coord2):\n",
    "    R = 6372800  # Earth radius in meters\n",
    "    lat1, lon1 = coord1\n",
    "    lat2, lon2 = coord2\n",
    "    \n",
    "    phi1, phi2 = math.radians(lat1), math.radians(lat2) \n",
    "    dphi       = math.radians(lat2 - lat1)\n",
    "    dlambda    = math.radians(lon2 - lon1)\n",
    "    \n",
    "    a = math.sin(dphi/2)**2 + \\\n",
    "        math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2\n",
    "    \n",
    "    return 2*R*math.atan2(math.sqrt(a), math.sqrt(1 - a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def coords_to_m_from_hpsc(coord):\n",
    "    hpsc_coord = (37.856668, -122.257166)\n",
    "    return haversine(coord, hpsc_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_dt(df):\n",
    "    dt = df.reset_index()['t'].astype(int).diff() * 1e-9\n",
    "    return dt.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_dt_and_velocity(df):\n",
    "    '''warning takes hella long\n",
    "    '''\n",
    "    df.loc[:,'dt'] =  get_dt(df)\n",
    "    df.index = df.index.tz_localize(None)\n",
    "\n",
    "    m_from_hpsc = []\n",
    "    for lat, lon in zip(df['lat'].values, df['lon'].values):\n",
    "        coord = (lat, lon)\n",
    "        d = coords_to_m_from_hpsc(coord)\n",
    "        m_from_hpsc.append(d)\n",
    "\n",
    "    df['m_from_hpsc'] = m_from_hpsc\n",
    "    df['km_from_hpsc'] = df['m_from_hpsc'] * 1e-3\n",
    "\n",
    "    df['dpos_hpsc'] = df['m_from_hpsc'].diff()\n",
    "\n",
    "    latlon = df.loc[:,['lat','lon']]\n",
    "    latlon_lag = latlon.shift(1)\n",
    "\n",
    "    dpos_ms = []\n",
    "    for i in tqdm(range(len(latlon))):\n",
    "        coord = (latlon.iloc[i]['lat'], latlon.iloc[i]['lon'])\n",
    "        prev_coord = (latlon_lag.iloc[i]['lat'], latlon_lag.iloc[i]['lon'])\n",
    "        dpos_ms.append(haversine(coord, prev_coord))\n",
    "        \n",
    "    df['dposm'] = dpos_ms\n",
    "    df['mpers'] = df['dposm'] / df['dt']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_is_trip_bool(\n",
    "        data, \n",
    "        min_vel=0, \n",
    "        max_vel=35,\n",
    "        max_dt=120,\n",
    "        max_dposm=300,\n",
    "    ):\n",
    "    moving = (min_vel <= data['mpers'])\n",
    "    not_too_fast = (data['mpers'] <= max_vel)\n",
    "    smooth_in_time = (data['dt'] <= max_dt)\n",
    "    smooth_in_space = (data['dposm'] <= max_dposm)\n",
    "    is_trip = moving & not_too_fast & smooth_in_time & smooth_in_space\n",
    "    return is_trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_trip_time_bools(data, is_trip):\n",
    "    ind_diff = is_trip.astype(int).diff().fillna(0)\n",
    "    trip_starts = (ind_diff == 1)\n",
    "    trip_ends = (ind_diff == -1)\n",
    "    trips = (ind_diff.apply(lambda x: max(x, 0)).cumsum() * is_trip).astype(int)\n",
    "    return trip_starts, trip_ends, trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def split_trips_into_lines(data):\n",
    "    # split into trips\n",
    "    trip_starts_ind = np.where(data['trip_starts'])[0]\n",
    "    trips = np.split(data.loc[:,['lon','lat',]].values, trip_starts_ind)\n",
    "    return trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_big_bay_lims():\n",
    "    lims = {}\n",
    "    lims['N'] = 38\n",
    "    lims['S'] = 37.6\n",
    "    lims['E'] = -122.15\n",
    "    lims['W'] = -122.6\n",
    "    return lims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_small_bay_lims():\n",
    "    lims = {}\n",
    "    lims['N'] = 37.9\n",
    "    lims['S'] = 37.75\n",
    "    lims['E'] = -122.2\n",
    "    lims['W'] = -122.34\n",
    "    return lims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def spatial_filter_df(df, lims):\n",
    "    ind = (df['lat'] < lims['N']) & (lims['S'] < df['lat']) \n",
    "    ind = ind & (lims['W'] < df['lon']) & (df['lon'] < lims['E'])\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_gram.ipynb.\n",
      "Converted 01_gram_scratch.ipynb.\n",
      "Converted 02_write.ipynb.\n",
      "Converted 03_quiver.ipynb.\n",
      "Converted 04_graph_grams.ipynb.\n",
      "Converted 05_blender_prep.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "h\n",
      "Converted 06_turtle_scratch.ipynb.\n",
      "Converted 07_location_history_parse.ipynb.\n",
      "Converted 08_location_hist_scratch.ipynb.\n",
      "Converted 09_location_hist_plots.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:good_robot]",
   "language": "python",
   "name": "conda-env-good_robot-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
